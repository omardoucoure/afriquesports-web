{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üéØ Mistral 7B Fine-Tuning for French Football Commentary\n",
        "\n",
        "**Goal**: Fine-tune Mistral 7B to generate human-quality French football commentary\n",
        "\n",
        "**Dataset**: 255 L'√âquipe commentary examples from CAN 2025 & Europa League\n",
        "\n",
        "**Hardware**: Google Colab T4 GPU (15GB VRAM)\n",
        "\n",
        "**Estimated Time**: 6-12 hours\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Setup\n",
        "\n",
        "**IMPORTANT**: Enable GPU Runtime\n",
        "1. Runtime ‚Üí Change runtime type\n",
        "2. Hardware accelerator ‚Üí T4 GPU\n",
        "3. Save"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install dependencies\n",
        "!pip install -q unsloth transformers datasets trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Mount Google Drive (to save checkpoints)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "checkpoint_dir = '/content/drive/MyDrive/mistral-commentary-checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "print(f\"‚úÖ Checkpoints will be saved to: {checkpoint_dir}\")"
      ],
      "metadata": {
        "id": "mount_drive"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÅ Upload Training Data\n",
        "\n",
        "Upload `mistral_training.jsonl` from your local machine:\n",
        "1. Click the folder icon on the left\n",
        "2. Click upload icon\n",
        "3. Select `mistral_training.jsonl` (136 KB)"
      ],
      "metadata": {
        "id": "upload_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load and verify training data\n",
        "import json\n",
        "\n",
        "# Load dataset\n",
        "with open('mistral_training.jsonl', 'r', encoding='utf-8') as f:\n",
        "    training_data = [json.loads(line) for line in f]\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(training_data)} training examples\")\n",
        "print(f\"\\nüìù Sample example:\")\n",
        "print(json.dumps(training_data[0], indent=2, ensure_ascii=False)[:500])"
      ],
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Format data for Mistral\n",
        "from datasets import Dataset\n",
        "\n",
        "def format_chat(example):\n",
        "    \"\"\"Format messages into Mistral chat format\"\"\"\n",
        "    messages = example['messages']\n",
        "    \n",
        "    # Mistral chat template\n",
        "    text = f\"\"\"<s>[INST] {messages[0]['content']}\n",
        "\n",
        "{messages[1]['content']} [/INST] {messages[2]['content']}</s>\"\"\"\n",
        "    \n",
        "    return {'text': text}\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "dataset = Dataset.from_list(training_data)\n",
        "dataset = dataset.map(format_chat, remove_columns=['messages'])\n",
        "\n",
        "print(f\"‚úÖ Formatted {len(dataset)} examples\")\n",
        "print(f\"\\nüìù Sample formatted text:\")\n",
        "print(dataset[0]['text'][:300])"
      ],
      "metadata": {
        "id": "format_data"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ Load Mistral 7B Model"
      ],
      "metadata": {
        "id": "model_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Load Mistral 7B with 4-bit quantization\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 512  # Sufficient for commentary\n",
        "dtype = None  # Auto-detect\n",
        "load_in_4bit = True  # 4-bit quantization for T4 GPU\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully\")\n",
        "print(f\"Model size: {model.get_memory_footprint() / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Configure LoRA (Low-Rank Adaptation)\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=128,  # High rank for capturing writing style\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"      # MLP layers\n",
        "    ],\n",
        "    use_rslora=True,  # Rank-stabilized LoRA\n",
        "    use_gradient_checkpointing=\"unsloth\"  # Memory optimization\n",
        ")\n",
        "\n",
        "print(\"‚úÖ LoRA configuration applied\")\n",
        "print(f\"Trainable parameters: {model.print_trainable_parameters()}\")"
      ],
      "metadata": {
        "id": "configure_lora"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üèãÔ∏è Training Configuration"
      ],
      "metadata": {
        "id": "training_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Configure training parameters\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    # Output\n",
        "    output_dir=checkpoint_dir,\n",
        "    \n",
        "    # Training schedule\n",
        "    num_train_epochs=3,  # 3 passes over the data\n",
        "    per_device_train_batch_size=2,  # Small batch for T4 GPU\n",
        "    gradient_accumulation_steps=8,  # Effective batch size: 16\n",
        "    \n",
        "    # Learning rate\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.1,  # 10% warmup\n",
        "    \n",
        "    # Optimization\n",
        "    optim=\"adamw_8bit\",  # Memory-efficient optimizer\n",
        "    fp16=True,  # Mixed precision training\n",
        "    \n",
        "    # Logging & checkpointing\n",
        "    logging_steps=10,\n",
        "    save_steps=200,  # Checkpoint every 200 steps (Colab can disconnect)\n",
        "    save_total_limit=3,  # Keep only last 3 checkpoints\n",
        "    \n",
        "    # Evaluation\n",
        "    eval_strategy=\"no\",  # No validation set (small dataset)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Training configuration:\")\n",
        "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"   Checkpoints: {checkpoint_dir}\")"
      ],
      "metadata": {
        "id": "training_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Initialize trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    args=training_args,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_text_field=\"text\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Trainer initialized\")\n",
        "print(f\"   Training examples: {len(dataset)}\")\n",
        "print(f\"   Steps per epoch: {len(dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n",
        "print(f\"   Total training steps: {trainer.state.max_steps}\")"
      ],
      "metadata": {
        "id": "init_trainer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Start Training\n",
        "\n",
        "**Expected duration**: 6-12 hours on T4 GPU\n",
        "\n",
        "**Monitor**:\n",
        "- Loss should decrease from ~2.0 to ~0.5\n",
        "- Checkpoints saved every 200 steps to Google Drive\n",
        "- If Colab disconnects, you can resume from the last checkpoint"
      ],
      "metadata": {
        "id": "train_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Train the model!\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"üöÄ Starting training...\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"‚úÖ Training complete!\")\n",
        "print(f\"   Duration: {elapsed / 3600:.2f} hours\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "train_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß™ Test the Model\n",
        "\n",
        "Generate sample commentary to verify quality"
      ],
      "metadata": {
        "id": "test_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Test generation\n",
        "FastLanguageModel.for_inference(model)  # Enable fast inference\n",
        "\n",
        "def generate_commentary(minute, event_type):\n",
        "    \"\"\"Generate football commentary\"\"\"\n",
        "    \n",
        "    event_type_fr = {\n",
        "        'goal': 'But',\n",
        "        'commentary': 'Commentaire g√©n√©ral',\n",
        "        'substitution': 'Remplacement',\n",
        "        'penalty': 'P√©nalty'\n",
        "    }.get(event_type, event_type)\n",
        "    \n",
        "    prompt = f\"\"\"<s>[INST] Tu es un commentateur sportif professionnel pour L'√âquipe, sp√©cialis√© dans le football. Ton style est vif, pr√©cis, √©motionnel mais jamais sensationnaliste. Tu varies ton vocabulaire et ta structure de phrases.\n",
        "\n",
        "G√©n√®re un commentaire de match pour:\n",
        "\n",
        "Minute: {minute}\n",
        "Type d'√©v√©nement: {event_type_fr}\n",
        "\n",
        "Commentaire: [/INST] \"\"\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    \n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=120,\n",
        "        temperature=0.9,\n",
        "        top_p=0.95,\n",
        "        top_k=50,\n",
        "        repetition_penalty=1.15,\n",
        "        do_sample=True\n",
        "    )\n",
        "    \n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract only the assistant's response\n",
        "    commentary = generated.split('[/INST]')[-1].strip()\n",
        "    \n",
        "    return commentary\n",
        "\n",
        "# Test examples\n",
        "test_cases = [\n",
        "    (\"45'+2\", \"goal\"),\n",
        "    (\"67'\", \"commentary\"),\n",
        "    (\"82'\", \"substitution\"),\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing model...\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for minute, event_type in test_cases:\n",
        "    commentary = generate_commentary(minute, event_type)\n",
        "    print(f\"\\n{minute} - {event_type}\")\n",
        "    print(f\"‚û°Ô∏è  {commentary}\")\n",
        "    print(\"-\" * 70)"
      ],
      "metadata": {
        "id": "test_generation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Save Model"
      ],
      "metadata": {
        "id": "save_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Save the fine-tuned model\n",
        "output_dir = \"/content/drive/MyDrive/mistral-commentary-final\"\n",
        "\n",
        "# Save LoRA adapter\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"‚úÖ Model saved to: {output_dir}\")\n",
        "print(f\"   Size: {sum(f.stat().st_size for f in Path(output_dir).rglob('*') if f.is_file()) / 1e6:.1f} MB\")"
      ],
      "metadata": {
        "id": "save_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Export to GGUF (Optional)\n",
        "\n",
        "For deployment with Ollama"
      ],
      "metadata": {
        "id": "export_section"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: Merge LoRA and export to GGUF\n",
        "!pip install -q llama-cpp-python\n",
        "\n",
        "# Merge LoRA adapter into base model\n",
        "merged_dir = \"/content/mistral-commentary-merged\"\n",
        "model.save_pretrained_merged(merged_dir, tokenizer, save_method=\"merged_16bit\")\n",
        "\n",
        "print(f\"‚úÖ Merged model saved to: {merged_dir}\")\n",
        "\n",
        "# Convert to GGUF (requires llama.cpp)\n",
        "# Note: This step requires additional setup\n",
        "print(\"\\n‚ö†Ô∏è  GGUF conversion:\")\n",
        "print(\"   1. Download merged model to local machine\")\n",
        "print(\"   2. Use llama.cpp convert.py to create GGUF\")\n",
        "print(\"   3. Upload to server: scp mistral-commentary-q4.gguf root@159.223.103.16:~/models/\")"
      ],
      "metadata": {
        "id": "export_gguf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Training Complete!\n",
        "\n",
        "### Next Steps:\n",
        "1. **Download model** from Google Drive: `mistral-commentary-final/`\n",
        "2. **Test locally** before deploying to production\n",
        "3. **Deploy to server** with A/B testing framework\n",
        "4. **Monitor quality** metrics (repetition, length variance)\n",
        "\n",
        "### Files in Google Drive:\n",
        "- `mistral-commentary-checkpoints/` - Training checkpoints\n",
        "- `mistral-commentary-final/` - Fine-tuned LoRA adapter\n",
        "- `mistral-commentary-merged/` - Merged 16-bit model\n",
        "\n",
        "### Expected Results:\n",
        "- ‚úÖ Natural French commentary (90-95% human-like)\n",
        "- ‚úÖ No repetitive phrases (repetition score < 0.25)\n",
        "- ‚úÖ Varied sentence length and structure\n",
        "- ‚úÖ Proper tense mixing (pr√©sent, pass√© compos√©)\n",
        "\n",
        "---\n",
        "\n",
        "**Training time**: ~6-12 hours on T4 GPU  \n",
        "**Model size**: ~4GB (GGUF q4_k_m)  \n",
        "**Dataset**: 255 L'√âquipe examples"
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}
